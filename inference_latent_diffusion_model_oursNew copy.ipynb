{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ff1a600",
   "metadata": {},
   "source": [
    "# This notebook is an example notebook to inference the latent diffusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae80fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd()+\"/ldm\")\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import torch\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "transform_PIL = T.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# yaml_path=\"/mayo_atlas/home/m288756/latent-diffusion-inpainting/ldm/models/ldm/inpainting_big/config.yaml\"\n",
    "# model_path=\"ldm/models/ldm/inpainting_big/last.ckpt\"\n",
    "# model_path=\"/mayo_atlas/home/m288756/latent-diffusion-inpainting/logs/checkpoints/last.ckpt\"\n",
    "\n",
    "yaml_path=\"/mayo_atlas/home/m288756/stable-diffusion/models/ldm/inpainting_big/config.yaml\"\n",
    "# model_path=\"/mayo_atlas/home/m288756/stable-diffusion/models/ldm/inpainting_big/last.ckpt\"\n",
    "# model_path=\"/mayo_atlas/home/m288756/latent-diffusion-inpainting/ldm/models/ldm/inpainting_big/new_model.ckpt\"\n",
    "# model_path=\"/mayo_atlas/home/m288756/stable-diffusion/logs2/checkpoints/last.ckpt\"\n",
    "# model_path=\"/mayo_atlas/home/m288756/stable-diffusion/logs/checkpoints/last2.ckpt\"\n",
    "model_path=\"/home/m288756/stable-diffusion/someCheckpoints/last1million1epoch.ckpt\"\n",
    "\n",
    "\n",
    "##create model\n",
    "def create_model(device):\n",
    "    \n",
    "    #load config and checkpoint\n",
    "    config = OmegaConf.load(yaml_path)\n",
    "    config.model['params']['ckpt_path']=model_path\n",
    "    \n",
    "    model = instantiate_from_config(config.model)\n",
    "    sampler = DDIMSampler(model)\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model,sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rectangle_mask(image):\n",
    "    # Create a copy of the original image\n",
    "    masked_image = image.copy()\n",
    "    image_x, image_y = image.size\n",
    "    \n",
    "    # width = random.randint(70, 150)\n",
    "    width = random.randint(300, 400)\n",
    "    print(width)\n",
    "    height = int(width * random.uniform(0.7, 1.5))\n",
    "    \n",
    "    \n",
    "    # Create a black background mask\n",
    "    mask = Image.new('L', (image_x, image_y), 0)\n",
    "    mask_small = mask.copy()\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "    \n",
    "\n",
    "    # Randomly generate the coordinates for the top-left and bottom-right corners of the rectangle\n",
    "    x1 = random.randint(0, image.size[0] - width)\n",
    "    y1 = random.randint(0, image.size[1] - height)\n",
    "    x2 = x1 + width\n",
    "    y2 = y1 + height\n",
    "\n",
    "    # Draw a white rectangle on the mask\n",
    "    draw.rectangle([x1, y1, x2, y2], fill=255)\n",
    "    \n",
    "    # Draw a white rectangle on the mask\n",
    "    # mask_small = mask.copy()\n",
    "    # draw2 = ImageDraw.Draw(mask_small)\n",
    "    # draw2.rectangle([x1+30, y1+30, x2-30, y2-30], fill=0)\n",
    "    \n",
    "    # draw2 = ImageDraw.Draw(mask_small)\n",
    "\n",
    "    # # Apply the mask to the original image\n",
    "    # masked_image.paste(Image.new('RGB', (image_x, image_y), (0, 0, 0)), mask=mask)\n",
    "    \n",
    "\n",
    "    return image, mask,x1+30, y1+30, x2-30, y2-30\n",
    "    # return image, mask, mask_small,x1+30, y1+30, x2-30, y2-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcad2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(region, percentage):\n",
    "        h, w, _ = region.shape\n",
    "        min_size = min(h, w)\n",
    "        new_h, new_w = int(min_size * percentage), int(min_size * percentage)\n",
    "        \n",
    "        top = random.randint(0, h - new_h)\n",
    "        left = random.randint(0, w - new_w)\n",
    "\n",
    "        cropped = region[top: top + new_h, left: left + new_w, :]\n",
    "\n",
    "        print(f'region after crop: {cropped.shape}')\n",
    "        return cropped\n",
    "\n",
    "def process_data(image,mask,size=512):\n",
    "\n",
    "    # creating a 3 dimensional mask\n",
    "    mask = np.array(mask)\n",
    "    mask = np.expand_dims(mask, axis=2)\n",
    "\n",
    "    # normalzie and transform the image into tensor\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "    image = cv2.resize(image, (size, size))\n",
    "    \n",
    "    # image_base = image.astype(np.float32) / 255.0#\n",
    "    image = image.astype(np.float32) / 255.0#\n",
    "    image_base = image.copy()\n",
    "    # image = image[None].transpose(0,3,1,2)\n",
    "    image = torch.from_numpy(image)\n",
    "\n",
    "    \n",
    "    # normalzie and transform the mask into tensor\n",
    "    mask = mask.astype(np.float32) / 255.0#\n",
    "    mask[mask < 0.1] = 0\n",
    "    mask[mask >= 0.1] = 1\n",
    "#     maskbase = mask[None].transpose(0,3,1,2)\n",
    "\n",
    "    \n",
    "    # produce the masked image by subtraction\n",
    "    # mask = mask[None].transpose(0,3,1,2)\n",
    "    maskbase = mask.copy()\n",
    "    mask = torch.from_numpy(mask)\n",
    "\n",
    "#     mask = torch.from_numpy(maskbase)\n",
    "#     image = image_base[None].transpose(0,3,1,2)\n",
    "#     image = torch.from_numpy(image)\n",
    "    # masked_image = (1 - mask) * image\n",
    "\n",
    "\n",
    "    # Find bounding box of the masked region\n",
    "    non_zero_indices = torch.nonzero(mask, as_tuple=False)\n",
    "    if non_zero_indices.numel() > 0:\n",
    "        min_yx = torch.min(non_zero_indices, dim=0)[0]\n",
    "        max_yx = torch.max(non_zero_indices, dim=0)[0]\n",
    "        min_y, min_x = min_yx[0], min_yx[1]\n",
    "        max_y, max_x = max_yx[0], max_yx[1]\n",
    "\n",
    "        # Ensure minimum width and height of 30 pixels\n",
    "        if max_y - min_y < 30:\n",
    "            center_y = (max_y + min_y) // 2\n",
    "            min_y = max(center_y - 15, 0)\n",
    "            max_y = min(center_y + 15, size - 1)\n",
    "\n",
    "        if max_x - min_x < 30:\n",
    "            center_x = (max_x + min_x) // 2\n",
    "            min_x = max(center_x - 15, 0)\n",
    "            max_x = min(center_x + 15, size - 1)\n",
    "\n",
    "        # Crop the region from the original image\n",
    "        cropped_region = image[min_y:max_y+1, min_x:max_x+1, :]\n",
    "    else:\n",
    "        # If there are no nonzero indices, crop a 30x30 region from the center of the mask location\n",
    "        center_y, center_x = size // 2, size // 2\n",
    "        min_y = max(center_y - 15, 0)\n",
    "        max_y = min(center_y + 15, size - 1)\n",
    "        min_x = max(center_x - 15, 0)\n",
    "        max_x = min(center_x + 15, size - 1)\n",
    "        # print(\"No nonzero indices crop -------------------\")\n",
    "\n",
    "        cropped_region = image[min_y:max_y+1, min_x:max_x+1, :]\n",
    "    \n",
    "    \n",
    "    print(f'cropped region {cropped_region.shape}')\n",
    "    print(f'image {image.shape}')\n",
    "    print(f'mask {mask.shape}')\n",
    "    \n",
    "    \n",
    "    # Randomly crop between 50% and 100% of the cropped_region\n",
    "    crop_percentage = random.uniform(0.5, 0.8)\n",
    "    # print(f'crop percentage: {crop_percentage}')\n",
    "    cropped_region = random_crop(cropped_region, crop_percentage)\n",
    "\n",
    "    # now resize the cropped_region to 512x512\n",
    "    cropped_region2 = cv2.resize(np.array(cropped_region), (size, size))\n",
    "\n",
    "    print(f'cropped region2 {cropped_region2.shape}')\n",
    "\n",
    "    # print(f'masked image {masked_image.shape}')\n",
    "\n",
    "\n",
    "    # image = image_base[None].transpose(0,3,1,2)\n",
    "    # image = torch.from_numpy(image)\n",
    "    # print(f'maskbase {maskbase.shape}')\n",
    "    # mask = maskbase[None].transpose(0,3,1,2)\n",
    "    mask = torch.from_numpy(maskbase)\n",
    "    # cropped_region2 = torch.from_numpy(cropped_region2)\n",
    "    cropped_region2 = torch.from_numpy(cropped_region2[None]) #.transpose(0,3,1,2)\n",
    "    \n",
    "\n",
    "    print(\"-----------------------------\")\n",
    "    print(f'cropped region2 {cropped_region2.shape}')\n",
    "    print(f'image {image.shape}')\n",
    "    print(f'mask {mask.shape}')\n",
    "    \n",
    "    image = image[None]\n",
    "    mask = mask[None]\n",
    "    \n",
    "    image = image.permute(0,3,1,2)\n",
    "    cropped_region2 = cropped_region2.permute(0,3,1,2)\n",
    "    mask = mask.permute(0,3,1,2)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"-----------------------------\")\n",
    "    print(f'cropped region2 {cropped_region2.shape}')\n",
    "    print(f'image {image.shape}')\n",
    "    print(f'mask {mask.shape}')\n",
    "\n",
    "    # batch = {\"image_tensor\": image, \"mask_tensor\": mask, \"masked_image_tensor\": masked_image}\n",
    "    batch = {\"image_tensor\": image, \"mask_tensor\": mask, \"masked_image_tensor\": cropped_region2}\n",
    "    \n",
    "    \n",
    "    for k in batch:\n",
    "        batch[k] = batch[k] * 2.0 - 1.0\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb082c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_roi(image,mask,tumer_patch,x1, y1, x2, y2,size=512):\n",
    "\n",
    "    # creating a 3 dimensional mask\n",
    "    mask = np.array(mask)\n",
    "    mask = np.expand_dims(mask, axis=2)\n",
    "\n",
    "    # normalzie and transform the image into tensor\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "    image = cv2.resize(image, (size, size))\n",
    "    image = image.astype(np.float32) / 255.0#\n",
    "    image = image[None].transpose(0,3,1,2)\n",
    "    image = torch.from_numpy(image)\n",
    "\n",
    "    \n",
    "    # normalzie and transform the image into tensor\n",
    "    tumer_patch = np.array(tumer_patch.convert(\"RGB\"))\n",
    "    tumer_patch = cv2.resize(tumer_patch, (size, size))\n",
    "    tumer_patch = tumer_patch.astype(np.float32) / 255.0#\n",
    "    tumer_patch = tumer_patch[None].transpose(0,3,1,2)\n",
    "    tumer_patch = torch.from_numpy(tumer_patch)\n",
    "    \n",
    "    \n",
    "    # crop the tumer patch with the coordinates x1, y1, x2, y2\n",
    "    tumer_patch_region = tumer_patch[:,:,y1:y2,x1:x2]\n",
    "    original_image = image.clone()\n",
    "    # original_image[:,:,y1:y2,x1:x2] = tumer_patch_region\n",
    "    \n",
    "    \n",
    "    # normalzie and transform the mask into tensor\n",
    "    mask = mask.astype(np.float32) / 255.0#\n",
    "    mask[mask < 0.1] = 0\n",
    "    mask[mask >= 0.1] = 1\n",
    "    mask = mask[None].transpose(0,3,1,2)\n",
    "\n",
    "    # produce the masked image by subtraction\n",
    "    mask = torch.from_numpy(mask)\n",
    "\n",
    "    masked_image = (1 - mask) * image\n",
    "    masked_image[:,:,y1:y2,x1:x2] = tumer_patch_region\n",
    "    \n",
    "    # roi = image * mask\n",
    "    # roi = tumer_patch * mask\n",
    "    roi = original_image * mask\n",
    "    # roi[:,:,y1:y2,x1:x2] = tumer_patch_region\n",
    "\n",
    "    batch = {\"image_tensor\": image, \"mask_tensor\": mask, \"masked_image_tensor\": masked_image, \"roi\": roi}\n",
    "    \n",
    "    for k in batch:\n",
    "        batch[k] = batch[k] * 2.0 - 1.0\n",
    "\n",
    "    return batch \n",
    "\n",
    "\n",
    "# def process_data_roi(image, mask, size=512, frame_width=70):\n",
    "#     # Create a 3-dimensional mask\n",
    "#     mask = np.array(mask)\n",
    "#     mask = np.expand_dims(mask, axis=2)\n",
    "\n",
    "#     # Normalize and transform the image into a tensor\n",
    "#     image = np.array(image.convert(\"RGB\"))\n",
    "#     image = cv2.resize(image, (size, size))\n",
    "#     image = image.astype(np.float32) / 255.0\n",
    "#     image = image[None].transpose(0, 3, 1, 2)\n",
    "#     image = torch.from_numpy(image)\n",
    "\n",
    "#     # Normalize and transform the mask into a tensor\n",
    "#     mask = mask.astype(np.float32) / 255.0\n",
    "#     mask[mask < 0.1] = 0\n",
    "#     mask[mask >= 0.1] = 1\n",
    "#     mask = mask[None].transpose(0, 3, 1, 2)\n",
    "#     mask = torch.from_numpy(mask)\n",
    "\n",
    "#     # Dilate the mask to create a frame\n",
    "#     kernel = np.ones((frame_width, frame_width), np.uint8)\n",
    "#     dilated_mask = cv2.dilate(mask.squeeze().numpy(), kernel, iterations=1)\n",
    "\n",
    "#     # Convert dilated mask back to tensor\n",
    "#     dilated_mask = torch.from_numpy(np.expand_dims(dilated_mask, axis=(0, 1)))\n",
    "\n",
    "#     # Produce the masked image by subtraction\n",
    "#     masked_image = (1 - dilated_mask) * image\n",
    "#     masked_image = (1 - mask) * masked_image\n",
    "\n",
    "#     # Extract the Region of Interest (ROI) using the dilated mask\n",
    "#     roi = image * dilated_mask\n",
    "#     roi = (1 - mask) * roi\n",
    "\n",
    "#     batch = {\"image_tensor\": image, \"mask_tensor\": mask, \"masked_image_tensor\": masked_image, \"roi\": roi}\n",
    "\n",
    "#     for k in batch:\n",
    "#         batch[k] = batch[k] * 2.0 - 1.0\n",
    "\n",
    "#     return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e3a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # load_image=Image.open(\"/mayo_atlas/home/m288756/stable-diffusion/data/oneImage/TCGA-22-A5C4-01Z-00-DX1.54058689-5CA5-4F92-B18A-86208C24C87D_18432_47104.jpg\") #.resize((512,512))\n",
    "    load_image=Image.open(\"/home/m288756/stable-diffusion/data/skin/NormalSkin_ARZ_115_HE_21510_8604.jpg\")\n",
    "    image_resized = load_image.copy().resize((512,512))\n",
    "    \n",
    "    tumer_patch=Image.open(\"/home/m288756/stable-diffusion/data/skin/Poor_Diff_93_ARZ_HE_18432_18432.jpg\")\n",
    "    tumer_patch = tumer_patch.copy().resize((512,512))\n",
    "\n",
    "    _,mask,x1, y1, x2, y2=random_rectangle_mask(image_resized)\n",
    "    # _, mask,x1, y1, x2, y2=random_rectangle_mask(image_resized)\n",
    "    \n",
    "    batch = process_data_roi(load_image,mask,tumer_patch,x1, y1, x2, y2,size=512)\n",
    "\n",
    "    image_tensor=batch[\"image_tensor\"]\n",
    "    mask_tensor=batch[\"mask_tensor\"]\n",
    "    masked_image_tensor=batch[\"masked_image_tensor\"]\n",
    "    roi=batch[\"roi\"]\n",
    "\n",
    "\n",
    "    # Save images in two rows using plt\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # First row: Original Image, Generated Mask, Masked Image\n",
    "    plt.subplot(2, 4, 1)\n",
    "    plt.imshow(((image_tensor + 1.0) / 2.0).squeeze().numpy().transpose(2,1,0))\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 4, 2)\n",
    "    plt.imshow(((masked_image_tensor + 1.0) / 2.0).squeeze().numpy().transpose(1,2,0))\n",
    "    plt.title('Masked Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 4, 3)\n",
    "    plt.imshow(((mask_tensor + 1.0) / 2.0).squeeze().numpy(), cmap='gray')\n",
    "    plt.title('Generated Mask')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 4, 4)\n",
    "    plt.imshow(((masked_image_tensor + 1.0) / 2.0).squeeze().numpy().transpose(1,2,0))\n",
    "    plt.title('Masked Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 4, 5)\n",
    "    plt.imshow(((image_tensor + 1.0) / 2.0).squeeze().numpy().transpose(2,1,0))\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 4, 6)\n",
    "    plt.imshow(((masked_image_tensor + 1.0) / 2.0).squeeze().numpy().transpose(1,2,0))\n",
    "    plt.title('Masked Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 4, 7)\n",
    "    plt.imshow(((mask_tensor + 1.0) / 2.0).squeeze().numpy(), cmap='gray')\n",
    "    plt.title('Generated Mask')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 4, 8)\n",
    "    plt.imshow(((roi + 1.0) / 2.0).squeeze().numpy().transpose(1,2,0))\n",
    "    plt.title('ROI Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.savefig(f\"sample_images_rows.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402a01d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model,sampler=create_model(device)\n",
    "# model.load_state_dict(torch.load('/mayo_atlas/home/m288756/stable-diffusion/models/ldm/inpainting_big/new_model.ckpt')['state_dict'])\n",
    "# model.load_state_dict(torch.load('/home/m288756/stable-diffusion/logs1millionMaskedBackgroundNoCrops/2024-01-03T11-16-27_config/checkpoints/trainstep_checkpoints/epoch=000000-step=000011999.ckpt')['state_dict'])\n",
    "model.load_state_dict(torch.load(\"/mayo_atlas/home/m288756/stable-diffusion/logs1millionMaskedBackgroundNoCrops/2024-01-03T11-16-27_config/checkpoints/trainstep_checkpoints/epoch=000002-step=000057999.ckpt\")['state_dict'])\n",
    "# model.load_state_dict(torch.load('/mayo_atlas/home/m288756/stable-diffusion/logs1millionMaskedBackgroundNoCrops/2024-01-03T11-16-27_config/checkpoints/last.ckpt')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_image=Image.open(\"/mayo_atlas/home/m288756/stable-diffusion/data/oneImage/TCGA-22-A5C4-01Z-00-DX1.54058689-5CA5-4F92-B18A-86208C24C87D_18432_47104.jpg\") #.resize((512,512))\n",
    "load_image=Image.open(\"/home/m288756/stable-diffusion/data/skin/NormalSkin_ARZ_115_HE_21510_8604.jpg\")\n",
    "load_image = load_image.copy().resize((512,512))\n",
    "# image,mask,masked_image,region,masked_BK=random_rectangle_mask(image_resized)\n",
    "# image,mask, _=random_rectangle_mask(load_image)\n",
    "# image,_, mask,=random_rectangle_mask(load_image)\n",
    "image,mask,x1, y1, x2, y2=random_rectangle_mask(load_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Inference\n",
    "\n",
    "# target = 'ROI'\n",
    "target = 'Masked Image'\n",
    "\n",
    "tumer_patch=Image.open(\"/home/m288756/stable-diffusion/data/skin/Poor_Diff_93_ARZ_HE_18432_18432.jpg\") #.resize((512,512))\n",
    "tumer_patch = tumer_patch.copy().resize((512,512))\n",
    "\n",
    "# convert PIL image into input Torch Tensor\n",
    "# batch=process_data(image,mask,size=512)\n",
    "batch = process_data_roi(image,mask,tumer_patch,x1, y1, x2, y2,size=512)\n",
    "\n",
    "mask_tensor=batch[\"mask_tensor\"]\n",
    "\n",
    "if target == 'ROI':\n",
    "    masked_image_tensor=batch[\"roi\"]\n",
    "else:\n",
    "    masked_image_tensor=batch[\"masked_image_tensor\"]\n",
    "\n",
    "\n",
    "# print(f'image_tensor {image_tensor.shape}'\n",
    "#       f'mask_tensor {mask_tensor.shape}'\n",
    "#       f'masked_image_tensor {masked_image_tensor.shape}')\n",
    "\n",
    "# encode masked image and concat downsampled mask\n",
    "c = model.cond_stage_model.encode(masked_image_tensor.to(device))\n",
    "\n",
    "# the mask is frst being downsampled\n",
    "cc = torch.nn.functional.interpolate(mask_tensor.to(device),\n",
    "                                    size=c.shape[-2:])\n",
    "# concat the masked image and downsampled mask\n",
    "c = torch.cat((c, cc), dim=1)\n",
    "shape = (c.shape[1]-1,)+c.shape[2:]\n",
    "\n",
    "print(f'c.shape {c.shape}')\n",
    "\n",
    "# diffusion process\n",
    "samples_ddim, _ = sampler.sample(S=50,\n",
    "                             conditioning=c,\n",
    "                             batch_size=c.shape[0],\n",
    "                             shape=shape,\n",
    "                             verbose=False)\n",
    "\n",
    "\n",
    "# decode the latent vector (output)\n",
    "x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "\n",
    "\n",
    "# denormalize the output\n",
    "predicted_image_clamped = torch.clamp((x_samples_ddim+1.0)/2.0,\n",
    "                            min=0.0, max=1.0)\n",
    "\n",
    "\n",
    "output_PIL=transform_PIL(predicted_image_clamped[0])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 6))  # Create a 1x3 grid for displaying 3 images\n",
    "\n",
    "# Display each image on a separate subplot\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "axes[1].imshow(mask)\n",
    "axes[1].set_title('Mask')\n",
    "\n",
    "# print(masked_image_tensor.squeeze().numpy().transpose(1,2,0))\n",
    "axes[2].imshow(((masked_image_tensor + 1.0) / 2.0).squeeze().numpy().transpose(1,2,0))\n",
    "axes[2].set_title(target)\n",
    "\n",
    "\n",
    "axes[3].imshow(output_PIL)\n",
    "axes[3].set_title('Inpainted Image')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb4b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_PIL=transform_PIL(predicted_image_clamped[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d915c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))  # Create a 1x3 grid for displaying 3 images\n",
    "\n",
    "# Display each image on a separate subplot\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title('original image')\n",
    "\n",
    "axes[1].imshow(mask)\n",
    "axes[1].set_title('masked image')\n",
    "\n",
    "axes[2].imshow(output_PIL)\n",
    "axes[2].set_title('Inpainted Image')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d671ec6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d8ee98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7862563b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
